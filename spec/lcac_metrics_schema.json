{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "LCAC Benchmark Metrics Schema",
  "description": "Official schema for LCAC reasoning integrity, drift resistance, and cognitive trust benchmark outputs.",
  "type": "object",
  "properties": {
    "timestamp": {
      "type": "string",
      "format": "date-time",
      "description": "UTC ISO-8601 timestamp of benchmark completion."
    },
    "model": {
      "type": "string",
      "description": "Name or identifier of the reasoning model, engine, or system tested."
    },
    "participant": {
      "type": "string",
      "description": "Organization, institution, or benchmark participant identifier."
    },
    "benchmark_version": {
      "type": "string",
      "description": "Version of the LCAC benchmark suite or Git commit hash."
    },
    "metrics": {
      "type": "object",
      "description": "Quantitative reasoning metrics collected from LCAC evaluation cycles.",
      "properties": {
        "drift_mean": {
          "type": "number",
          "description": "Average drift magnitude across test intervals."
        },
        "drift_std": {
          "type": "number",
          "description": "Standard deviation of drift across evaluations."
        },
        "stability_mean": {
          "type": "number",
          "description": "Mean stability score of reasoning output consistency."
        },
        "stability_std": {
          "type": "number",
          "description": "Standard deviation of stability across test samples."
        },
        "latency_mean": {
          "type": "number",
          "description": "Average reasoning latency (seconds)."
        },
        "latency_std": {
          "type": "number",
          "description": "Standard deviation of reasoning latency (seconds)."
        },
        "trust_index": {
          "type": "number",
          "description": "Composite trust index combining drift and stability (0â€“1).",
          "minimum": 0,
          "maximum": 1
        }
      },
      "required": ["drift_mean", "stability_mean", "trust_index"]
    },
    "verdict": {
      "type": "string",
      "enum": ["Stable / High Trust", "Monitor Drift", "Unstable / Low Trust"],
      "description": "Final qualitative assessment of model reasoning integrity."
    },
    "notes": {
      "type": "string",
      "description": "Optional runtime or environment annotations."
    }
  },
  "required": ["timestamp", "model", "participant", "metrics", "verdict"],
  "additionalProperties": false
}
