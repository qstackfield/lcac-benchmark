{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "LCAC Benchmark Result Schema",
  "description": "Defines the standard structure for LCAC benchmark results measuring reasoning drift, stability, and trust index integrity.",
  "type": "object",
  "required": [
    "timestamp",
    "model",
    "participant",
    "benchmark_version",
    "metrics",
    "trust_index",
    "verdict"
  ],
  "properties": {
    "timestamp": {
      "type": "string",
      "format": "date-time",
      "description": "UTC ISO-8601 timestamp when the benchmark completed."
    },
    "model": {
      "type": "string",
      "description": "Name or identifier of the reasoning model or engine being tested."
    },
    "participant": {
      "type": "string",
      "description": "Institution, organization, or participant submitting this benchmark result."
    },
    "benchmark_version": {
      "type": "string",
      "description": "Version of the LCAC benchmark suite or Git commit hash used during evaluation."
    },
    "metrics": {
      "type": "object",
      "description": "Quantitative reasoning performance metrics.",
      "required": ["drift_mean", "drift_std", "stability", "latency_mean", "latency_std"],
      "properties": {
        "drift_mean": {
          "type": "number",
          "description": "Average reasoning drift magnitude across test cycles."
        },
        "drift_std": {
          "type": "number",
          "description": "Standard deviation of reasoning drift across iterations."
        },
        "stability": {
          "type": "number",
          "description": "Mean stability score of logical and contextual consistency (0–1).",
          "minimum": 0,
          "maximum": 1
        },
        "latency_mean": {
          "type": "number",
          "description": "Average response latency observed during evaluation (seconds)."
        },
        "latency_std": {
          "type": "number",
          "description": "Standard deviation of response latency."
        }
      }
    },
    "trust_index": {
      "type": "number",
      "description": "Derived composite trust index combining stability and drift metrics (0–1).",
      "minimum": 0,
      "maximum": 1
    },
    "verdict": {
      "type": "string",
      "enum": ["Stable / High Trust", "Monitor Drift", "Unstable / Low Trust"],
      "description": "Qualitative classification of benchmark result based on reasoning integrity."
    },
    "meta": {
      "type": "object",
      "description": "Supplementary metadata about the benchmark environment.",
      "properties": {
        "samples": {
          "type": "integer",
          "description": "Number of reasoning samples evaluated."
        },
        "environment": {
          "type": "string",
          "description": "Environment label (e.g., local, CI/CD, research, production)."
        },
        "runtime": {
          "type": "string",
          "description": "Execution runtime identifier (e.g., Python3)."
        },
        "lcac_suite": {
          "type": "string",
          "description": "Suite name or test group (e.g., open, institutional, enterprise)."
        }
      }
    }
  },
  "additionalProperties": false
}
